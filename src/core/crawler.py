from typing import List, Set, Callable, Optional
from urllib.parse import urlparse
from src.core.entities import Video
from src.core.ports import VideoScanner, LinkExtractor
import time


class Crawler:
    """
    Web sitesini Ã¶rÃ¼mcek aÄŸÄ± gibi gezerek (Crawling) video arayan sÄ±nÄ±f.
    """

    def __init__(self, scanner: VideoScanner, link_extractor: LinkExtractor):
        self.scanner = scanner
        self.link_extractor = link_extractor
        # Ziyaret edilmiÅŸ olan linkler (iÅŸlemleri bitmiÅŸ)
        self.visited_urls: Set[str] = set()
        # Bulunan videolarÄ±n temizlenmiÅŸ URL'lerini burada tutacaÄŸÄ±z (HafÄ±za)
        self.seen_video_urls: Set[str] = set()
        # GÃ¶rÃ¼len Video BaÅŸlÄ±klarÄ± (AynÄ± isimli videolarÄ± engellemek iÃ§in
        self.seen_titles: Set[str] = set()

    def _get_clean_url(self, url: str) -> str:
        """
        URL'i parametrelerden (?token=...) arÄ±ndÄ±rÄ±r ve normalize eder.
        BÃ¶ylece video.mp4?a=1 ile video.mp4?a=2 aynÄ± sayÄ±lÄ±r.
        """
        try:
            parsed = urlparse(url)
            # Sadece scheme (https), netloc (site.com) ve path (/video.mp4) kÄ±smÄ±nÄ± al
            clean = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
            return clean
        except:
            return url

    def start_crawling(
        self,
        start_url: str,
        max_depth: int = 3,
        max_pages: int = 100,
        max_videos: int = 100,
        progress_callback: Optional[Callable[[str, dict], None]] = None,
    ) -> List[Video]:
        """
        Belirtilen URL'den baÅŸlar ve max_depth kadar derine inerek tarar.
        Args:
            start_url: BaÅŸlangÄ±Ã§ adresi.
            max_depth: Ne kadar derine inileceÄŸi (0: Sadece ana sayfa, 1: Ana sayfa + linkleri).
        """
        # region TanÄ±mlamalar

        all_videos: List[Video] = []  # toplanan bÃ¼tÃ¼n videolarÄ± tutar.
        start_domain = urlparse(start_url).netloc.replace("www", "")

        # HafÄ±zalarÄ± sÄ±fÄ±rla
        self.visited_urls.clear()
        self.seen_video_urls.clear()
        self.seen_titles.clear()

        queued_urls: Set[str] = {
            start_url
        }  # KuyruÄŸa daha Ã¶nce eklenmiÅŸ linkleri tutar.  visited_urls'den farkÄ±: HenÃ¼z ziyaret edilmemiÅŸ ama sÄ±rada bekleyenleri de bilir.
        queue = [(start_url, 0)]  # iÅŸ kuyruÄŸu : URL ve derinlik
        pages_visited_count = 0  # Gezilen sayfa sayacÄ±

        # TanÄ±mlama: Emit Fonksiyonu -- UI mesaj gÃ¶ndermeyi kolaylaÅŸtÄ±ran yardÄ±mcÄ± bir fonksiyon
        def emit(type_str, message, extra_data=None):
            if progress_callback:
                payload = {
                    "message": message,
                    "stats": {
                        "pages": pages_visited_count,
                        "videos": len(all_videos),
                        "queue": len(queue),
                        "max_pages": max_pages,
                    },
                }
                if extra_data:
                    payload.update(extra_data)
                progress_callback(type_str, payload)
            time.sleep(0.02)

        emit(
            "log",
            f"ğŸ•·ï¸ Ã–rÃ¼mcek Modu BaÅŸladÄ±: {start_url} (Derinlik: {max_depth} (Limit: {max_pages} Sayfa))",
        )

        print(f"--> Ä°zin Verilen Ana Domain: {start_domain} ")

        # endregion

        while queue:
            time.sleep(0.1)  # sisteme dinlenme sÃ¼resi vermek iÃ§in
            # --- 1. GÃ¼venlik Limitleri ---
            # region GÃ¼venlik TanÄ±mlamalarÄ±
            # GÃœVENLÄ°K FRENÄ°: EÄŸer Ziyaret edilecek sayfa limit dolduysa dur!
            if pages_visited_count >= max_pages:
                emit("warning", f"ğŸ›‘ Sayfa limiti ({max_pages}) doldu.")
                break

            # GÃœVENLÄ°K FRENÄ°: EÄŸer Toplanacak max video limiti dolduysa dur!
            if len(all_videos) >= max_videos:
                emit("success", f"ğŸ‰ Video limiti ({max_videos}) doldu.")
                break

            current_url, current_depth = queue.pop(0)

            # Ziyaret kontrolÃ¼
            if current_url in self.visited_urls:
                continue

            self.visited_urls.add(current_url)
            pages_visited_count += 1

            # DURUM GÃœNCELLEME (Log olarak basma, sadece status gÃ¼ncelle)
            emit(
                "status",
                f"TaranÄ±yor ({pages_visited_count}/{max_pages}): {current_url}",
            )
            # endregion

            # 2. Video Tara
            # region Video Tarama
            try:
                found = self.scanner.scan(current_url)
                if found:
                    new_videos_count = 0
                    new_videos_in_page = []

                    emit(
                        "status",
                        f"Ziyaret Ediliyor ({pages_visited_count}/{max_pages})",
                        {"url": current_url},
                    )
                    # all_videos.extend(found)
                    for v in found:
                        # --- TEKÄ°LLÄ°K KONTROLÃœ (GELÄ°ÅMÄ°Å) ---
                        clean_url = self._get_clean_url(v.url)

                        # BaÅŸlÄ±k TemizliÄŸi (BoÅŸluklarÄ± sil, kÃ¼Ã§Ã¼k harfe Ã§evir) ---
                        clean_title = v.title.strip().lower() if v.title else ""

                        # EÄŸer bu temiz URL daha Ã¶nce gÃ¶rÃ¼lmediyse ekle
                        if (clean_url not in self.seen_video_urls) and (
                            clean_title not in self.seen_titles
                        ):
                            self.seen_video_urls.add(clean_url)
                            if clean_title:
                                self.seen_titles.add(clean_title)
                            all_videos.append(v)
                            new_videos_in_page.append(v.title)
                            new_videos_count += 1

                        # # video daha Ã¶nce eklendi mi kontorlÃ¼
                        # if not any(
                        #     existing.url == v.url for existing in all_videos
                        # ):  ## existing ==> Generator Expression -- geÃ§ici bir deÄŸiÅŸken (placeholder)
                        #     all_videos.append(v)
                        #     new_videos_count += 1
                        #     new_videos_in_page.append(v.title)

                    if new_videos_count > 0:
                        # Sadece yeni video varsa LOG bas
                        emit(
                            "video_found",
                            f"âœ… {new_videos_count} Yeni Video: {', '.join(new_videos_in_page)[:50]}...",
                        )

                else:
                    print(f"   âšª Video yok.")
            except Exception as e:
                emit("error", f"âš ï¸Tarama HatasÄ±: ({current_url}) : {str(e)}")

            # Derinlik limiti kontrolÃ¼
            if current_depth >= max_depth:
                print("   ğŸ›‘ Derinlik limitine ulaÅŸÄ±ldÄ±, link aranmayacak.")
                continue
            # endregion

            # 3. LÄ°NK TOPLAMA (LinkExtractor kullanÄ±mÄ±)
            # region Link Toplama
            try:
                links = self.link_extractor.extract_links(current_url)
                print(f"   ğŸ”— Sayfadaki Link SayÄ±sÄ±: {len(links)}")
                # new_links_count = 0
                max_len_links = 500
                for link in links:
                    # GÃ¼venlik1 : Kuyruk Ã§ok ÅŸiÅŸerse yeni link alma
                    if len(queue) > max_len_links:
                        break

                    # ---URL Normalizasyonu---
                    # 1. Her ÅŸeyi HTTPS yap(http ile https aynÄ± yerdir.)
                    if link.startswith("http://"):
                        link = link.replace("http://", "https://")

                    # 2.Sondaki gereksiz slash iÅŸaretini sil(site.com/a/ -> site.com/a gibi)
                    link = link.rstrip("/")

                    # 3. Fragment(yorumlar ) kÄ±smÄ± temizleme
                    if "#" in link:
                        link = link.split("#")[0]

                    # Linkin domainini al ve www.'yu sil
                    link_domain = urlparse(link).netloc.replace("www", "")

                    # Domain KontrolÃ¼ (Daha esnek)
                    if start_domain in link_domain:
                        # Hem ziyaret edilmemiÅŸ hem de kuyrukta yoksa ekle
                        if link not in self.visited_urls and link not in queued_urls:
                            queue.append((link, current_depth + 1))
                            queued_urls.add(
                                link
                            )  # ArtÄ±k bunu kuyruÄŸa attÄ±k, not alÄ±yoruz
                            # new_links_count += 1
                        else:
                            # FarklÄ± site (Ã–rn: facebook, twitter linkleri)
                            # print(f"   ğŸš« DÄ±ÅŸ Link: {link_domain}") # Ã‡ok kirlilik yapmasÄ±n diye kapalÄ±
                            pass
                    # if new_links_count > 0:
                    #     emit(
                    #         "log",
                    #         f"ğŸ”— Bu sayfadan {new_links_count} yeni link kuyruÄŸa eklendi.",
                    #     )

            except Exception as e:
                emit("error", f"   âš ï¸ Link HatasÄ±: {e}")
                continue
        # endregion

        emit(
            "finish",
            f"ğŸ --- CRAWLER BÄ°TTÄ° (Toplam Video: {len(all_videos)}, Toplam Saysa: {pages_visited_count}) ---\n",
        )
        return all_videos
